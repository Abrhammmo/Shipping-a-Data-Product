{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0a247a",
   "metadata": {},
   "source": [
    "## Task-3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205a94a",
   "metadata": {},
   "source": [
    "### Data Enrichment with Object Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5558c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add parent directory to path so we can import from src\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53ba9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image root: c:\\Users\\hp\\Desktop\\python-projects\\Shipping-a-Data-Product\\data\\raw\\images\n",
      "Output CSV: ../data/processed/yolo_detections.csv\n"
     ]
    }
   ],
   "source": [
    "from src.config import DATA_PATHS\n",
    "\n",
    "IMAGE_ROOT = DATA_PATHS[\"raw_images\"]\n",
    "OUTPUT_CSV = \"../data/processed/yolo_detections.csv\"\n",
    "\n",
    "print(\"Image root:\", IMAGE_ROOT)\n",
    "print(\"Output CSV:\", OUTPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba6330c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Paths validated and output directory ready\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "assert Path(IMAGE_ROOT).exists(), \"‚ùå Raw image directory does not exist\"\n",
    "\n",
    "Path(\"../data/processed\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Paths validated and output directory ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c69ceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ultralytics available\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    print(\"‚úÖ ultralytics available\")\n",
    "except ImportError:\n",
    "    print(\"Installing ultralytics...\")\n",
    "    !pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b66f4156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Available schemas:\n",
      "   - public\n",
      "   - raw\n",
      "\n",
      "üîç Looking for message-related tables...\n",
      "\n",
      "üìä Found related tables:\n",
      "   - public.fct_messages (30,714 rows)\n",
      "   - public.stg_telegram_messages (30,714 rows)\n",
      "   - raw.telegram_messages (34,474 rows)\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check what schemas and tables exist in the database\n",
    "import psycopg2\n",
    "from src.config import DATABASE_CONFIG\n",
    "\n",
    "def check_database_structure():\n",
    "    \"\"\"Check what schemas and tables exist in the database\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DATABASE_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Get all schemas\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT schema_name \n",
    "            FROM information_schema.schemata \n",
    "            WHERE schema_name NOT IN ('information_schema', 'pg_catalog', 'pg_toast')\n",
    "            ORDER BY schema_name\n",
    "        \"\"\")\n",
    "        schemas = cur.fetchall()\n",
    "        print(\"üìã Available schemas:\")\n",
    "        for schema in schemas:\n",
    "            print(f\"   - {schema[0]}\")\n",
    "        \n",
    "        # Check for message-related tables in each schema\n",
    "        print(\"\\nüîç Looking for message-related tables...\")\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT table_schema, table_name \n",
    "            FROM information_schema.tables \n",
    "            WHERE table_name LIKE '%message%' OR table_name LIKE '%fct%' OR table_name LIKE '%stg%'\n",
    "            ORDER BY table_schema, table_name\n",
    "        \"\"\")\n",
    "        tables = cur.fetchall()\n",
    "        \n",
    "        if tables:\n",
    "            print(\"\\nüìä Found related tables:\")\n",
    "            for schema, table in tables:\n",
    "                # Get row count\n",
    "                try:\n",
    "                    cur.execute(f'SELECT COUNT(*) FROM \"{schema}\".\"{table}\"')\n",
    "                    count = cur.fetchone()[0]\n",
    "                    print(f\"   - {schema}.{table} ({count:,} rows)\")\n",
    "                except:\n",
    "                    print(f\"   - {schema}.{table} (could not count rows)\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è No message-related tables found\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return schemas, tables\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking database: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "schemas, tables = check_database_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3987cf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Found 3 potential tables:\n",
      "   - public.fct_messages\n",
      "   - public.stg_telegram_messages\n",
      "   - raw.telegram_messages\n",
      "‚úÖ Found and loaded from public.fct_messages\n",
      "‚úÖ Loaded 29021 message keys from database\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from src.config import DATABASE_CONFIG\n",
    "\n",
    "def get_message_keys_from_db():\n",
    "    \"\"\"Fetch message_id -> (channel_key, date_key) mapping from database\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DATABASE_CONFIG)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # First, discover what tables actually exist\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT table_schema, table_name \n",
    "            FROM information_schema.tables \n",
    "            WHERE (table_name LIKE '%message%' OR table_name LIKE '%fct%' OR table_name LIKE '%stg%')\n",
    "            AND table_schema NOT IN ('information_schema', 'pg_catalog')\n",
    "            ORDER BY table_schema, table_name\n",
    "        \"\"\")\n",
    "        available_tables = cur.fetchall()\n",
    "        \n",
    "        if not available_tables:\n",
    "            print(\"‚ö†Ô∏è No message-related tables found in database\")\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            return {}\n",
    "        \n",
    "        print(f\"üìã Found {len(available_tables)} potential tables:\")\n",
    "        for schema, table in available_tables:\n",
    "            print(f\"   - {schema}.{table}\")\n",
    "        \n",
    "        results = None\n",
    "        schema_used = None\n",
    "        \n",
    "        # Try to find fct_messages first (if dbt has run)\n",
    "        for schema_name, table_name in available_tables:\n",
    "            if table_name in ['fct_messages', 'fct_message']:\n",
    "                try:\n",
    "                    # Check if it has the required columns\n",
    "                    cur.execute(f\"\"\"\n",
    "                        SELECT column_name \n",
    "                        FROM information_schema.columns \n",
    "                        WHERE table_schema = %s AND table_name = %s\n",
    "                        AND column_name IN ('message_id', 'channel_key', 'date_key')\n",
    "                    \"\"\", (schema_name, table_name))\n",
    "                    columns = [row[0] for row in cur.fetchall()]\n",
    "                    if len(columns) == 3:\n",
    "                        cur.execute(f\"\"\"\n",
    "                            SELECT message_id, channel_key, date_key\n",
    "                            FROM \"{schema_name}\".\"{table_name}\"\n",
    "                        \"\"\")\n",
    "                        results = cur.fetchall()\n",
    "                        schema_used = f\"{schema_name}.{table_name}\"\n",
    "                        print(f\"‚úÖ Found and loaded from {schema_used}\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        # If fct_messages doesn't exist, try staging tables\n",
    "        if results is None:\n",
    "            print(\"‚ö†Ô∏è fct_messages not found, trying staging tables...\")\n",
    "            for schema_name, table_name in available_tables:\n",
    "                if 'stg' in table_name.lower() or 'staging' in table_name.lower() or 'raw' in table_name.lower():\n",
    "                    try:\n",
    "                        # Check if it has required columns\n",
    "                        cur.execute(f\"\"\"\n",
    "                            SELECT column_name \n",
    "                            FROM information_schema.columns \n",
    "                            WHERE table_schema = %s AND table_name = %s\n",
    "                            AND column_name IN ('message_id', 'channel_name', 'message_date')\n",
    "                        \"\"\", (schema_name, table_name))\n",
    "                        columns = [row[0] for row in cur.fetchall()]\n",
    "                        if 'message_id' in columns and 'channel_name' in columns and 'message_date' in columns:\n",
    "                            print(f\"   Trying to compute keys from {schema_name}.{table_name}...\")\n",
    "                            cur.execute(f\"\"\"\n",
    "                                WITH channel_keys AS (\n",
    "                                    SELECT \n",
    "                                        ROW_NUMBER() OVER (ORDER BY channel_name) AS channel_key,\n",
    "                                        channel_name\n",
    "                                    FROM (\n",
    "                                        SELECT DISTINCT channel_name \n",
    "                                        FROM \"{schema_name}\".\"{table_name}\"\n",
    "                                    ) t\n",
    "                                ),\n",
    "                                date_keys AS (\n",
    "                                    SELECT DISTINCT\n",
    "                                        TO_CHAR(message_date, 'YYYYMMDD')::INT AS date_key,\n",
    "                                        DATE(message_date) AS full_date\n",
    "                                    FROM \"{schema_name}\".\"{table_name}\"\n",
    "                                )\n",
    "                                SELECT \n",
    "                                    s.message_id,\n",
    "                                    ck.channel_key,\n",
    "                                    dk.date_key\n",
    "                                FROM \"{schema_name}\".\"{table_name}\" s\n",
    "                                JOIN channel_keys ck ON s.channel_name = ck.channel_name\n",
    "                                JOIN date_keys dk ON DATE(s.message_date) = dk.full_date\n",
    "                            \"\"\")\n",
    "                            results = cur.fetchall()\n",
    "                            schema_used = f\"{schema_name}.{table_name}\"\n",
    "                            print(f\"‚úÖ Computed keys from {schema_used}\")\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        print(f\"   ‚ö†Ô∏è Could not use {schema_name}.{table_name}: {str(e)[:100]}\")\n",
    "                        continue\n",
    "        \n",
    "        if results is None:\n",
    "            print(\"‚ö†Ô∏è Could not find suitable tables for channel_key and date_key\")\n",
    "            print(\"   Will proceed with None values for these fields\")\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            return {}\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "        # Create mapping: message_id -> (channel_key, date_key)\n",
    "        message_map = {}\n",
    "        for message_id, channel_key, date_key in results:\n",
    "            message_map[str(message_id)] = (channel_key, date_key)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(message_map)} message keys from database\")\n",
    "        return message_map\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load from database: {e}\")\n",
    "        print(\"   Will proceed without channel_key and date_key\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "message_keys_map = get_message_keys_from_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c118c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ YOLO detection function ready\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "from src.yolo.classifier import classify_image\n",
    "from src.yolo.utils import extract_channel_and_message_id\n",
    "import csv\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "def run_yolo_with_keys(image_root: Path, output_csv: str, limit=None, message_keys_map=None):\n",
    "    \"\"\"Run YOLO detection pipeline with channel_key and date_key enrichment\"\"\"\n",
    "    if message_keys_map is None:\n",
    "        message_keys_map = {}\n",
    "    \n",
    "    results_rows = []\n",
    "    images = list(image_root.rglob(\"*.jpg\"))\n",
    "    \n",
    "    if limit:\n",
    "        images = images[:limit]\n",
    "        print(f\"üì∏ Processing first {limit} images for sample...\")\n",
    "    else:\n",
    "        print(f\"üì∏ Processing {len(images)} images...\")\n",
    "    \n",
    "    for idx, image_path in enumerate(images, 1):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"   Progress: {idx}/{len(images)}\")\n",
    "        \n",
    "        channel_code, message_id = extract_channel_and_message_id(image_path)\n",
    "        \n",
    "        # Run YOLO detection\n",
    "        detections = model(image_path, verbose=False)[0]\n",
    "        \n",
    "        detected_objects = [\n",
    "            (model.names[int(box.cls)], float(box.conf))\n",
    "            for box in detections.boxes\n",
    "        ]\n",
    "        \n",
    "        # Get channel_key and date_key from database mapping\n",
    "        channel_key = None\n",
    "        date_key = None\n",
    "        if message_id in message_keys_map:\n",
    "            channel_key, date_key = message_keys_map[message_id]\n",
    "        \n",
    "        if not detected_objects:\n",
    "            # Still write a row even if no detections, but with \"none\" class\n",
    "            image_category = \"none\"\n",
    "            results_rows.append({\n",
    "                \"message_id\": message_id,\n",
    "                \"channel_key\": channel_key,\n",
    "                \"date_key\": date_key,\n",
    "                \"detected_class\": \"none\",\n",
    "                \"confidence_score\": 0.0,\n",
    "                \"image_category\": image_category\n",
    "            })\n",
    "        else:\n",
    "            image_category = classify_image(detected_objects)\n",
    "            \n",
    "            for label, confidence in detected_objects:\n",
    "                results_rows.append({\n",
    "                    \"message_id\": message_id,\n",
    "                    \"channel_key\": channel_key,\n",
    "                    \"date_key\": date_key,\n",
    "                    \"detected_class\": label,\n",
    "                    \"confidence_score\": round(confidence, 4),\n",
    "                    \"image_category\": image_category\n",
    "                })\n",
    "    \n",
    "    # Write results\n",
    "    fieldnames = [\"message_id\", \"channel_key\", \"date_key\", \"detected_class\", \"confidence_score\", \"image_category\"]\n",
    "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results_rows)\n",
    "    \n",
    "    print(f\"‚úÖ Saved {len(results_rows)} detection records to {output_csv}\")\n",
    "    return results_rows\n",
    "\n",
    "print(\"‚úÖ YOLO detection function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b21e456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ Processing first 5 images for sample...\n",
      "‚úÖ Saved 7 detection records to ../data/processed/yolo_detections_sample.csv\n",
      "\n",
      "‚úÖ Sample processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Process first 5 images for sample\n",
    "SAMPLE_OUTPUT = \"../data/processed/yolo_detections_sample.csv\"\n",
    "\n",
    "sample_results = run_yolo_with_keys(\n",
    "    image_root=IMAGE_ROOT,\n",
    "    output_csv=SAMPLE_OUTPUT,\n",
    "    limit=5,\n",
    "    message_keys_map=message_keys_map\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Sample processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "430af923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Sample Results (First 5 images):\n",
      "\n",
      "================================================================================\n",
      " message_id  channel_key  date_key detected_class  confidence_score  image_category\n",
      "      53469            1  20251116       scissors            0.3423           other\n",
      "      53490            1  20251123         person            0.7094       lifestyle\n",
      "      53490            1  20251123         person            0.5319       lifestyle\n",
      "      53490            1  20251123         person            0.3156       lifestyle\n",
      "      53491            1  20251123           none            0.0000            none\n",
      "      53492            1  20251123           book            0.4193           other\n",
      "      53493            1  20251127         bottle            0.7497 product_display\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Total rows in sample: 7\n",
      "‚úÖ Columns: ['message_id', 'channel_key', 'date_key', 'detected_class', 'confidence_score', 'image_category']\n"
     ]
    }
   ],
   "source": [
    "# Display sample results\n",
    "import pandas as pd\n",
    "\n",
    "sample_df = pd.read_csv(SAMPLE_OUTPUT)\n",
    "print(\"üìä Sample Results (First 5 images):\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(sample_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ Total rows in sample: {len(sample_df)}\")\n",
    "print(f\"‚úÖ Columns: {list(sample_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "166de145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ Processing 15494 images...\n",
      "   Progress: 100/15494\n",
      "   Progress: 200/15494\n",
      "   Progress: 300/15494\n",
      "   Progress: 400/15494\n",
      "   Progress: 500/15494\n",
      "   Progress: 600/15494\n",
      "   Progress: 700/15494\n",
      "   Progress: 800/15494\n",
      "   Progress: 900/15494\n",
      "   Progress: 1000/15494\n",
      "   Progress: 1100/15494\n",
      "   Progress: 1200/15494\n",
      "   Progress: 1300/15494\n",
      "   Progress: 1400/15494\n",
      "   Progress: 1500/15494\n",
      "   Progress: 1600/15494\n",
      "   Progress: 1700/15494\n",
      "   Progress: 1800/15494\n",
      "   Progress: 1900/15494\n",
      "   Progress: 2000/15494\n",
      "   Progress: 2100/15494\n",
      "   Progress: 2200/15494\n",
      "   Progress: 2300/15494\n",
      "   Progress: 2400/15494\n",
      "   Progress: 2500/15494\n",
      "   Progress: 2600/15494\n",
      "   Progress: 2700/15494\n",
      "   Progress: 2800/15494\n",
      "   Progress: 2900/15494\n",
      "   Progress: 3000/15494\n",
      "   Progress: 3100/15494\n",
      "   Progress: 3200/15494\n",
      "   Progress: 3300/15494\n",
      "   Progress: 3400/15494\n",
      "   Progress: 3500/15494\n",
      "   Progress: 3600/15494\n",
      "   Progress: 3700/15494\n",
      "   Progress: 3800/15494\n",
      "   Progress: 3900/15494\n",
      "   Progress: 4000/15494\n",
      "   Progress: 4100/15494\n",
      "   Progress: 4200/15494\n",
      "   Progress: 4300/15494\n",
      "   Progress: 4400/15494\n",
      "   Progress: 4500/15494\n",
      "   Progress: 4600/15494\n",
      "   Progress: 4700/15494\n",
      "   Progress: 4800/15494\n",
      "   Progress: 4900/15494\n",
      "   Progress: 5000/15494\n",
      "   Progress: 5100/15494\n",
      "   Progress: 5200/15494\n",
      "   Progress: 5300/15494\n",
      "   Progress: 5400/15494\n",
      "   Progress: 5500/15494\n",
      "   Progress: 5600/15494\n",
      "   Progress: 5700/15494\n",
      "   Progress: 5800/15494\n",
      "   Progress: 5900/15494\n",
      "   Progress: 6000/15494\n",
      "   Progress: 6100/15494\n",
      "   Progress: 6200/15494\n",
      "   Progress: 6300/15494\n",
      "   Progress: 6400/15494\n",
      "   Progress: 6500/15494\n",
      "   Progress: 6600/15494\n",
      "   Progress: 6700/15494\n",
      "   Progress: 6800/15494\n",
      "   Progress: 6900/15494\n",
      "   Progress: 7000/15494\n",
      "   Progress: 7100/15494\n",
      "   Progress: 7200/15494\n",
      "   Progress: 7300/15494\n",
      "   Progress: 7400/15494\n",
      "   Progress: 7500/15494\n",
      "   Progress: 7600/15494\n",
      "   Progress: 7700/15494\n",
      "   Progress: 7800/15494\n",
      "   Progress: 7900/15494\n",
      "   Progress: 8000/15494\n",
      "   Progress: 8100/15494\n",
      "   Progress: 8200/15494\n",
      "   Progress: 8300/15494\n",
      "   Progress: 8400/15494\n",
      "   Progress: 8500/15494\n",
      "   Progress: 8600/15494\n",
      "   Progress: 8700/15494\n",
      "   Progress: 8800/15494\n",
      "   Progress: 8900/15494\n",
      "   Progress: 9000/15494\n",
      "   Progress: 9100/15494\n",
      "   Progress: 9200/15494\n",
      "   Progress: 9300/15494\n",
      "   Progress: 9400/15494\n",
      "   Progress: 9500/15494\n",
      "   Progress: 9600/15494\n",
      "   Progress: 9700/15494\n",
      "   Progress: 9800/15494\n",
      "   Progress: 9900/15494\n",
      "   Progress: 10000/15494\n",
      "   Progress: 10100/15494\n",
      "   Progress: 10200/15494\n",
      "   Progress: 10300/15494\n",
      "   Progress: 10400/15494\n",
      "   Progress: 10500/15494\n",
      "   Progress: 10600/15494\n",
      "   Progress: 10700/15494\n",
      "   Progress: 10800/15494\n",
      "   Progress: 10900/15494\n",
      "   Progress: 11000/15494\n",
      "   Progress: 11100/15494\n",
      "   Progress: 11200/15494\n",
      "   Progress: 11300/15494\n",
      "   Progress: 11400/15494\n",
      "   Progress: 11500/15494\n",
      "   Progress: 11600/15494\n",
      "   Progress: 11700/15494\n",
      "   Progress: 11800/15494\n",
      "   Progress: 11900/15494\n",
      "   Progress: 12000/15494\n",
      "   Progress: 12100/15494\n",
      "   Progress: 12200/15494\n",
      "   Progress: 12300/15494\n",
      "   Progress: 12400/15494\n",
      "   Progress: 12500/15494\n",
      "   Progress: 12600/15494\n",
      "   Progress: 12700/15494\n",
      "   Progress: 12800/15494\n",
      "   Progress: 12900/15494\n",
      "   Progress: 13000/15494\n",
      "   Progress: 13100/15494\n",
      "   Progress: 13200/15494\n",
      "   Progress: 13300/15494\n",
      "   Progress: 13400/15494\n",
      "   Progress: 13500/15494\n",
      "   Progress: 13600/15494\n",
      "   Progress: 13700/15494\n",
      "   Progress: 13800/15494\n",
      "   Progress: 13900/15494\n",
      "   Progress: 14000/15494\n",
      "   Progress: 14100/15494\n",
      "   Progress: 14200/15494\n",
      "   Progress: 14300/15494\n",
      "   Progress: 14400/15494\n",
      "   Progress: 14500/15494\n",
      "   Progress: 14600/15494\n",
      "   Progress: 14700/15494\n",
      "   Progress: 14800/15494\n",
      "   Progress: 14900/15494\n",
      "   Progress: 15000/15494\n",
      "   Progress: 15100/15494\n",
      "   Progress: 15200/15494\n",
      "   Progress: 15300/15494\n",
      "   Progress: 15400/15494\n",
      "‚úÖ Saved 34114 detection records to ../data/processed/yolo_detections.csv\n",
      "\n",
      "‚úÖ Full processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Now process ALL images\n",
    "FINAL_OUTPUT = \"../data/processed/yolo_detections.csv\"\n",
    "\n",
    "all_results = run_yolo_with_keys(\n",
    "    image_root=IMAGE_ROOT,\n",
    "    output_csv=FINAL_OUTPUT,\n",
    "    limit=None,  # Process all images\n",
    "    message_keys_map=message_keys_map\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Full processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ea4c250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results Summary:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>channel_key</th>\n",
       "      <th>date_key</th>\n",
       "      <th>detected_class</th>\n",
       "      <th>confidence_score</th>\n",
       "      <th>image_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20251116.0</td>\n",
       "      <td>scissors</td>\n",
       "      <td>0.3423</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20251123.0</td>\n",
       "      <td>person</td>\n",
       "      <td>0.7094</td>\n",
       "      <td>lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20251123.0</td>\n",
       "      <td>person</td>\n",
       "      <td>0.5319</td>\n",
       "      <td>lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20251123.0</td>\n",
       "      <td>person</td>\n",
       "      <td>0.3156</td>\n",
       "      <td>lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53491</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20251123.0</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   message_id  channel_key    date_key detected_class  confidence_score  \\\n",
       "0       53469          1.0  20251116.0       scissors            0.3423   \n",
       "1       53490          1.0  20251123.0         person            0.7094   \n",
       "2       53490          1.0  20251123.0         person            0.5319   \n",
       "3       53490          1.0  20251123.0         person            0.3156   \n",
       "4       53491          1.0  20251123.0           none            0.0000   \n",
       "\n",
       "  image_category  \n",
       "0          other  \n",
       "1      lifestyle  \n",
       "2      lifestyle  \n",
       "3      lifestyle  \n",
       "4           none  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display summary of final results\n",
    "final_df = pd.read_csv(FINAL_OUTPUT)\n",
    "\n",
    "print(\"Final Results Summary:\")\n",
    "print(\"=\"*80)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc25006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ YOLO detection module loaded\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Force reload the modules\n",
    "for module in ['src.yolo.detector', 'src.yolo.utils', 'src.yolo.classifier']:\n",
    "    if module in sys.modules:\n",
    "        importlib.reload(sys.modules[module])\n",
    "\n",
    "from src.yolo.detector import run_yolo_pipeline\n",
    "\n",
    "print(\"‚úÖ YOLO detection module loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1db234d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CSV schema validated\n"
     ]
    }
   ],
   "source": [
    "expected_cols = {\n",
    "    \"message_id\",\n",
    "    \"channel_key\",\n",
    "    \"detected_class\",\n",
    "    \"confidence_score\",\n",
    "    \"image_category\"\n",
    "}\n",
    "\n",
    "assert expected_cols.issubset(final_df.columns), \"‚ùå CSV schema invalid\"\n",
    "\n",
    "print(\"‚úÖ CSV schema validated\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
